\chapter{Supplementary Results} % Main appendix title
\label{app:suppresults}
\section{Semantic Embeddings}
\subsection{Principle Component Analysis of Embeddings}

To examine the internal structure of different embeddings, we performed PCA and plot the eigenvalues of each PC. Refer to Figure \ref{fig:EngDecorVarRatio} for English spaces, Figure \ref{fig:FreDecorVarRatio} for French ones. 

\begin{figure}
    \centering
    \makebox[\linewidth]{
    \includegraphics[scale=.8]{Figures/EngDecorVarRatio.pdf}
    }
    \caption[EVR of 4 Semantic Spaces, English]{PCA of 4 English semantic spaces of lexicon size 8157. \textbf{Left panel}: The projected \code{SIM} onto \code{MIX} (\code{SIG}) has larger variance than \code{SIM}. The suppression of \code{SIG} from \code{MIX} has little impact on \code{MIX}'s variance. \textbf{Right panel}: \code{SIG} and \code{SIM} have a denser variance concentrated on first PCs, while \code{ASN} and \code{MIX} have more homogeneous variance distributions. These are indications that \code{ASN} and \code{MIX} possess rich semantic information compared to \similarity spaces.} 
    \label{fig:EngDecorVarRatio}
\end{figure}
\begin{figure}
    \centering
    \makebox[\linewidth]{
    \includegraphics[scale=0.8]{Figures/FreDecorVarRatio.pdf}
    }
    \caption[EVR of 4 Semantic Spaces, French]{PCA of 4 French semantic spaces of lexicon size 24519. \textbf{Left panel}: Due to the poor linear correlation found between \code{SIM} and \code{MIX}, the variance of \code{SIG} is systematically smaller than the other three spaces, the original space \code{SIM} has larger variances. The suppression of \code{SIG} from \code{MIX} has little impact on the model's variance. \textbf{Right panel}: \code{SIG} has a denser variance concentrated on first PCs, while the other three spaces have more homogeneous variance distributions.\label{fig:FreDecorVarRatio}} 

\end{figure}


\subsection{Visualization of Semantic Spaces}
\label{appsubsec:projectorvisu}

The English \code{SIM}, French \code{SIM} and \code{ASN} spaces are visualized with Tensorflow Embedding Projector. While the English \code{SIM} (Figure \ref{fig:engSIMwalk}), built with \code{WordNet}, is still homogeneously distributed in the span of the first 3 principle vectors, the French \code{SIM} (Figure \ref{fig:freSIMmarcher}) is dominated by POS information. The four first PCs each denotes the dimension of nouns, verbs, adjectives and adverbs. Despite the data source of French \code{WOLF} being \code{WordNet}s of other languages, many of the semantic links are lost.

French \code{ASN} (Figure \ref{fig:freASNmarcher}) on the contrary, has no substantial influence by POS. 

\begin{figure}
    \centering
    \includegraphics[clip, trim=1cm 1.5cm 2cm 0.2cm, scale=1]{Figures/EngSIMPCAforwalk.pdf}
    \caption[English \code{SIM} Space Visualization]{Each point represent a word, coordinated in the first 3 PC of the English \code{SIM} space. The points are regularly distributed in a sphere, with several local clusters centered by POS tags. The red sphere is the location of \emph{walk}. The color gradient denotes the vectorial proximity of the words with \emph{walk}. Lighter colors denotes a larger similarity.}
    \label{fig:engSIMwalk}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \makebox[.5\linewidth]{
        \includegraphics[clip, trim=1cm 1.5cm 2cm 0.2cm, scale=1]{Figures/FreSIMPCAformarcher_v.pdf}
        }
        \caption[French \code{SIM} Space Visualization]{Due to the nature of \code{WOLF}, 
        \code{SIM}'s first PCs denotes POS category. Light colors indicate the proximity of the represented words with \emph{marcher\_v}. All of the semantic neighbors of \emph{marcher} are verbs.}
        \label{fig:freSIMmarcher}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \makebox[.5\linewidth]{
        \includegraphics[clip, trim=1cm 1.5cm 2cm 0.2cm, scale=1]{Figures/FreASNPCAformarcher_v.pdf}
        }
        \caption[French \code{ASN} Space Visualization]{\code{ASN}'s variance are more homogeneously distributed over PCs.}
        \label{fig:freASNmarcher}
    \end{minipage}
\end{figure}

\subsection{Semantic Ranking Task Results}
\label{appsubsec:wnembeddingtests}

In the first stage of the project, we tested different combinations of semantic relations to maximize \similarity scores while minimize \association. The different iteration scores are reported in Table \ref{apptab:engWordNetIteration}. 

Along the trials with \code{WordNet}, another synonym database created based on thesauri is also tested with the same algorithm. The word-pair proximity ranking task indicates that associational relations can still be deducted from the synonyms. However, the \textbf{SIMLEX-999} score reported by this data source approaches the state-of-the-art of single-source language models\footnote{Task dashboard: \url{https://fh295.github.io/simlex.html}.}. 

\input{Tables/engWordNetIteration}

\subsection{Example of Semantic Neighbours in French Embeddings} 

No systematic control for \code{ASN} is performed for the \citetitle{desaint-exuperyPetitPrince1943} story. We taken several examples and examined the semantic neighbours with Tensorflow Projector. Table \ref{tab:freNeighbour} gives an example and nature of the semantic neighbours in four embedding spaces conforms with the hypothesized properties of the semantic principles (syntagmatic and paradigmatic).

\input{Tables/freProfesseur}


\subsection{Vocabulary Coverage by POS}

Additional fMRI encoding regressions have been trained with POS-specific design matrices to reveal potential differences of semantic processing. The statistics for the POS-specific corpus is reported in Table \ref{apptab:lppcoverage}.
\input{Tables/lppcoveragePOS}


\section{Non-nested Model Comparison}
\label{appsubsec:nonnestedcompres}

\subsubsection{Use \code{SIM} to Predict \code{ASN}}

\begin{figure}
    \centering
        \makebox[.5\linewidth]{
            \includegraphics[scale=.8]{Figures/SIM_ASN_MODEL_COMP.pdf}
            }
            \caption[Coefficients of \texttt{SIM} Columns Predicting \texttt{ASN}]{The most effective column-models of \code{ASN} are positioned as posterior as 47, 47 then 30. Only first 35 dimensions including 3 shared non-embedding ones are visualized. No particular significant column-to-column correlation is suggested by the coefficients.} 
            \label{fig:SIMASNMODELCOMP}
\end{figure}

Each of the 203 columns in the \code{ASN} class design matrix (including non-semantic features) are predicted by 103 columns of the \code{SIM} class design matrix. The average trained coefficients are plotted in Figure \ref{fig:SIMASNMODELCOMP}. We averaged the \code{r2} score of each column model across 9 cross-validation runs. The histogram of the scores are plotted in Figure \ref{fig:SIMASNDist}, informative model scores are presented in Table \ref{tab:SIMASNScores}. As our design-matrices are orthonormalized, columns sitting at larger indexes have a dependency on smaller-indexed columns. The first columns being bell predicted start at index 12 (to 14). Other columns are scattered up to index 47. We can therefore conclude that the predictability found are purely due to numerical coincidences.

\begin{figure}
    \centering
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \makebox[.5\linewidth]{
            \includegraphics[scale=.8]{Figures/SIMASNDist.pdf}
            }
            \caption[\texttt{SIM} Predicted \texttt{ASN} Column Model]{3 columns are perfectly predicted as they are non-semantic features that are shared by the design matrices. For the rest most of the column-models are worse than arbitrary models.} 
            \label{fig:SIMASNDist}
    \end{minipage}%
    \begin{minipage}[t]{.5\textwidth}
        \centering
        \makebox[.5\linewidth]{
            \includegraphics[scale=.8]{Figures/ASNSIMDist.pdf}
            }
            \caption[\texttt{ASN} Predicted \texttt{SIM} Column Model]{Most of the column-models are worse than arbitrary models. More columns in \code{SIM} are better predicted with \code{ASN} design matrices.} 
            \label{fig:ASNSIMDist}
    \end{minipage}
\end{figure}


\begin{table}
    \centering
    \begin{ThreePartTable}
    \begin{tabularx}{\textwidth}{p{1.7cm} *{11}{L}}
    \mc{12}{l}{\tabhead{\texttt{SIM} Predicted \texttt{ASN} Column Model Performances}} \\
    \toprule
    \tabhead{Col Index} & 22 & 20 & 14 & 37 & 47 & 18 & 27 & 26 & 13 & 12 & 30 \\ 
    \midrule 
    \tabhead{\code{r2}} & .0891 & .0834 & .0548 & .0346 & .0164 & .0152 & .0122 & .0113 & .0112 & .0075 & .0004 \\  
    \tabhead{\code{r}} & .2984 & .2887 & .2340 & .1861 & .1279 & .1233 & .1104 & .1064 & .1058 & .0867 & .0203 \\
    \bottomrule
    \end{tabularx}
    \end{ThreePartTable}
    \caption[\texttt{SIM} Predicted \texttt{ASN} Column Model Performances]{Index starts at 0, \code{ASN} group features starts from 3.  Among the 14 informative models (\code{r2} > 0), 3 are non-semantic features (not listed above). Pearson's \code{r} are converted from \code{r2}. \label{tab:SIMASNScores}}
\end{table}


\subsubsection{Use \code{ASN} to Predict \code{SIM}}

\begin{table}
    \centering
    \begin{ThreePartTable}
    \begin{tabularx}{\textwidth}{p{1.7cm} *{11}{L}}
    \mc{12}{l}{\tabhead{\texttt{ASN} Predicted \texttt{SIM} Column Model Performances}} \\
    \toprule
    \tabhead{Col Index} & 6 & 3 & 5 & 11 & 4 & 7 & 75 & 18 & 42 & 25 & 47 \\  
    \midrule 

    \tabhead{\code{r2}} & .2761 & .2646 & .2041 & .1256 & .1032 & .0971 & .0529 & .0485 & .0192 & .0158 & .0107 \\  
    \tabhead{\code{r}} & .5254 & .5144 & .4518 & .3544 & .3213 & .3116 & .2299 & .2202 & .1385 & .1256 & .1037 \\
    \bottomrule
    \end{tabularx}
    \end{ThreePartTable}
    \caption[\texttt{ASN} Predicted \texttt{SIM} Column Model Performances]{Index starts at 0, \code{SIM} group features starts from 3. \code{SIM} columns are much better predicted by \code{ASN}. \label{tab:ASNSIMScores}}
\end{table}

\begin{figure}
    \centering
        \makebox[.5\linewidth]{
            \includegraphics[scale=.8]{Figures/ASN_SIM_MODEL_COMP.pdf}
            }
            \caption[Coefficients of \texttt{ASN} Columns Predicting \texttt{SIM}]{The first 7 \code{SIM} columns are well predicted by \code{ASN} columns. The visualization suggests that the first 40 columns of \code{ASN} \emph{regressor class} correlated with the first 5 \code{SIM}-\emph{group} \emph{regressors}.} 
            \label{fig:ASNSIMMODELCOMP}

\end{figure}

The same procedure yields also 14 effective column models for \code{SIM}. The correlation coefficients are significantly higher than the models predicted with \code{SIM} matrices. Besides, the first 5 columns of \code{SIM} are all well predicted (\code{r} > 0.30) by \code{ASN}, indicating there's partial signal information overlap between the two models. Since Section \ref{appsubsec:projectorvisu} shows that the first 4 dimensions in \code{SIM} one-hot encode POS information, it is reasonable that POS information is also traceable in syntagmatic-information dominated semantic embedding.

To further investigate the column-wise correlation, we also plot the coefficients of each \code{ASN} column learned by GLM (Figure \ref{fig:SIMASNMODELCOMP}). 

\section{Regression}
\subsection{More on \(\alpha\) and Dimension Selection}
\label{appsubsec:alphadim}

\subsubsection{Completeness of Research Space}

For illustrative purpose, we selected four typical voxels in post-hoc from the regression results of \code{MIX}, run 1 subject 1 (Figure \ref{fig:TypicalVoxelDistributionS1R1})\footnote{Interactive version of the plot available online \url{https://plot.ly/~neegola/11/}.}. \code{MIX} is used since it is the default semantic space used in other works and has not been modified. Each voxel is the best modeled voxel who maximizes \code{r2} among all \(\alpha\)s using only partial feature information of a certain regressor class. For example, a \code{CWRATE} class typical voxel is a voxel of which the best \code{r2} is achieved with \emph{all} first 3 features (\code{dim=3}). 

\begin{figure}
    \centering
    \makebox[\linewidth]{
    \includegraphics[scale=1]{Figures/TypicalVoxelDistributionS1R1.pdf}
    }
    \caption[Typical-Voxels' Response to \(\alpha\) and Dimension]{A selection of typical voxels found in \code{MIX} model regression results of subject 1 run 1. All trails with dimensionality and \(\alpha\) values are visualized. The title of each subplot corresponds to the voxel's best dimension profile. As shown by all four voxels, the regularization by large \(\alpha\) is beneficial only in higher feature dimensions. Our selection of \(\alpha\) contains a near-optimal value candidates for these voxels since the curve is all declining for the largest values. Green plots indicate the top 10 best configurations of the given voxel.} 
    \label{fig:TypicalVoxelDistributionS1R1}
\end{figure}

The optimal \code{r2}s of typical voxels are never attained at the upper bound of \(\alpha\) and dimension space. We plotted the heat-map for all voxels from the same run to verify if it is also the case at the whole-brain level (e.g. Figure \ref{fig:MIX_HeatmapAlphaDimS1R1}). We averaged 9 cross-validation run results to visualize subject-level best configuration distribution (Figure \ref{fig:MIX_HeatmapAlphaDimS1R0}). Histograms of best dimension and \(\alpha\) voxel-configuration of the averaged results are also plotted in Figure \ref{fig:MIX_CountPlotAlphaDimS1R0}. Plots for all runs and all subjects are available online\footnote{\code{MIX}: \url{http://bit.ly/micipsa_mix_heatmap}. \code{SIM}: \url{http://bit.ly/micipsa_sim_heatmap}. \code{SIG}: \url{http://bit.ly/micipsa_sig_heatmap}. \code{ASN}: \url{http://bit.ly/micipsa_asn_heatmap}.}. The analysis confirms that our parameter combination test range contains the near-optimal configuration for each voxel.

\begin{figure}
    \centering
    
    \makebox[.5\linewidth]{
            \includegraphics[scale=.8]{Figures/MIX_HeatmapAlphaDimS1R1.pdf}
            }
            \caption[Session Best Hyper-parameter Configuration Voxel-Count Heat-map]{The scores are retrieved from \code{MIX} model regression for subject 1, fMRI run 1. Each cell represents an \(\alpha\) and dimension combination. The color indicates the logarithm of number of voxels having its global optimality with a given parameter set after having filtered out non-informative voxels (\code{r2} < 0). For small dimensions (< 35), small \(\alpha\)s (including 0) achieve the best performance. Starting from dimension 35, Ridge regularization with larger \(\alpha\)s is necessary.} 
            \label{fig:MIX_HeatmapAlphaDimS1R1}
   
\end{figure}

\begin{figure}
    \centering
    \makebox[\linewidth]{
    \includegraphics[scale=1]{Figures/MIX_CountPlotAlphaDimS1R0.pdf}
    }
    \caption[Best Hyper-parameter Configuration Voxel-Count Histograms]{\textbf{R0} denotes that the subject average is plotted. \textbf{Left panel}: Among informative voxels, a large portion of voxels are best modeled by \code{RMS} and \code{CWRATE} regressor classes. The addition of each semantic dimension from \code{MIX} improved a small proportion of voxel-models, marginal might be the contribution. \textbf{Right panel}: Most of the voxels are best modeled without Ridge regularization. The larger number obtained at \(\alpha=10000\) might indicate that larger alpha values might help better model a thousand voxels. A cumulation of voxel-count at the upper bound of the \(\alpha\) axis is noted: we performed post-hoc test for larger \(\alpha\) values than in the initial configuration, but the improvement of \code{r2} over the original score is marginal (\(<10^{-4}\)) for a sample of these voxels. A post-hoc analysis of larger \(\alpha\)s indicates a limited improvement of \code{r2}, thus for computational simplicity we kept the original Grid Search space of \(\alpha\).\label{fig:MIX_CountPlotAlphaDimS1R0}} 
    
\end{figure}

\subsubsection{Dimension Variability across Voxels}

At obtention of regression results, we visualized the contrast of voxel-model scores if we enforce the dimensionality of design matrix and do not perform the nested-model substitution\footnote{Histograms and scatter plots available for the first 3 runs of first 3 subjects. File name ending with \code{alpha\_best} indicates the plotted results are among the voxel-wise best alpha. \url{http://bit.ly/micipsa_dimension_variability}.}. The results indicate that fixing the semantic model dimensionality at full overfits most of the voxels.

The regularization parameter has an overall penalization effect: on increasing \(\alpha\), the overall distribution of \code{r2} is more densely centered around 0. 


\section{Embedding Model Brain Maps}
\subsection{Nested Model Cluster Tables}

For each regression model, the top 1\% and 0.1\% voxels are selected to analyze the spatial pattern of voxel-models (Table \ref{tab:rmsCluters}). 
The Wilcoxon test results for nested-model \code{r2} improvements are also clustered. The voxels are thresholded with statistical significance. Refer to Table \ref{tab:cwrateImprovementClusters} for \code{CWRATE} improvements, Table \ref{tab:simImprovementClusters} for \code{SIM}, Table \ref{tab:sigImprovementClusters} for \code{SIG} and Table \ref{tab:asnImprovementClusters} for \code{ASN}. Corresponding F-test visualization data for all nested contrasts are reported in Table \ref{tab:Ftest}.

\input{Tables/rmsClusters}
\input{Tables/cwrateImprovementClusters}
\input{Tables/simImprovementClusters}
\input{Tables/sigImprovementClusters}
\input{Tables/asnImprovementClusters}
\input{Tables/F}

\subsection{\emph{Similarity}\slash \emph{Association} Contrast}
\label{subsec:sig}

Two \similarity spaces are tested. The \code{SIM}-\code{ASN} contrast voxel-clusters are reported in Tables \ref{tab:simasnContrastClusters_sim} and \ref{tab:simasnContrastClusters_asn}. The \code{SIG}-\code{ASN} contrast is presented in Figure \ref{fig:EMB_SIG_ASN_ContrastMapG}, Tables \ref{tab:sigasnContrastClusters_sig} and \ref{tab:sigasnContrastClusters_asn}, with ROI analysis in Figure \ref{fig:SIG_ASN_ROI}.

\input{Tables/sim_asnContrastCluster}
\input{Tables/sig_asnContrastCluster}

\begin{figure}
    \centering
    % \begin{minipage}[t]{\textwidth}
    \makebox[\linewidth]{
    \includegraphics[width=.4\paperwidth]{Figures/EMB_SIG_ASN_r2_ContrastMapG.pdf}
    % % \includegraphics[width=.4\paperwidth, clip, trim=0cm,0cm,0cm, 1cm]{Figures/EMB_SIM_ASN_ContrastMapG.pdf}
    }
    % \end{minipage}
    % \begin{minipage}[t]{\textwidth}
    \makebox[\linewidth]{
        \includegraphics[clip, trim=0cm 0cm 0cm 1cm, width=.4\paperwidth]{Figures/EMB_SIG_ASN_ContrastMapG.pdf}
    }
    % \end{minipage}
    \caption[\code{SIG}-\code{ASN} Contrast, Group]{The differences of best voxel-model \code{r2}s are plotted in \textbf{upper panel}, significance levels in \textbf{lower panel}.} 
    \label{fig:EMB_SIG_ASN_ContrastMapG}
\end{figure}


\begin{figure}
    \centering
    \makebox[\linewidth]{
    \includegraphics[width=\paperwidth]{Figures/SIG_ASN_ROI.pdf}
    }
    \caption[\code{SIG} \code{ASN} ROI Contrast, Group]{*: 0.05 uncorrected, ***: 0.05 ROI-wise multi-comparison corrected. Blue asterisks for \code{SIG}, red for \code{ASN}.\\ The average \code{r2} of voxels in a ROI is computed. We select only ROIs with scores>0.015 in either of \code{SIG} and \code{ASN} models. ROIs are of minimum size of 26 voxels (radius of 7 mm). Multiple ROIs in bilateral mSTG are found preferential for \code{SIG}, \code{ASN} are mostly inferotemporal (including fusiform and hippocampal), anterotemporal (in TP and MTG) and inferofrontal (in pars opercularis and triangularis) regions.} 
    \label{fig:SIG_ASN_ROI}
\end{figure}
