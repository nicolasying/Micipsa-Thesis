\chapter{Introduction} 

\label{chap:introduction} 

%----------------------------------------------------------------------------------------
\section{Semantic Memory, Representation and Processing} 

Semantics as in linguistic context, is the connection between language forms such as orthography, syntax, and meanings including lexical and phrasal ones. The brain processes semantics in aide of semantic memory \parencite{tulvingEpisodicSemanticMemory1972}, of which the loci in human cognition system are still actively being debated. 

Semantic memory is unlike episodic memory, which is individual-specific and modality dependent. As semantic memory is associated with general world knowledge \parencite{mcraeSemanticMemory2013}, semantic memory tends to be shared across individuals within a common cultural background. Such invariance provides a window for semantic memory loci localization in the human brain. 

In \citeauthor{tulvingEpisodicSemanticMemory1972}'s view, semantic memory is conceptually dissociated, but not necessarily functionally or structurally separated from procedural and episodic memory. However, studies \parencite{vargha-khademDifferentialEffectsEarly1997} suggest the structural and functional dissociation of episodic memory and semantic memory, and that parahippocampal cortices, as a classical theoretical locus of episodic memory, are not crucial for the normal functioning of semantic memory.
    

While some theorist argue for the temporal localization of semantic memory~\parencite{saumierSemanticMemory2002, martinSemanticMemoryBrain2001}, recent studies found neural correlates of semantic knowledge distributed in multiple lobes (refer to Table \ref{tab:distributedareastudysynthesis} for a resume), supporting thus the distributed hypothesis that semantic knowledge might be encoded in multiple brain areas. The cortices involved in semantic processing other than the temporal cortex, are strongly associated to perceptual, sensorial and\slash or affectual functions, suggesting that semantic memory depends on episodic events~\parencite{moseleyNounsVerbsObjects2014}. 

A theoretical reconciliation between these two schools is the specialization of cortical areas implicated in semantic tasks: the abstract, amodal semantic memory being grounded by concrete modal episodic memories \parencite{pecherGroundingCognitionRole2005}. 

The classical semantic memory locus could serve as the convergence zone for binding information from modality-specific cortices \parencite{damasioNeuralBasisLexical1996, damasioNeuralSystemsWord2004, simmonsSimilarityintopographyPrincipleReconciling2003}. Anatomical evidences, that the possible loci sit at the convergence of multiple perceptual processing streams \parencite{binderNeurobiologySemanticMemory2011}, support this theory. \textcite{pattersonWhereYouKnow2007} further exploited \citeauthor{damasioNeuralSystemsWord2004}'s arguments by proposing the \emph{Hub-and-Spoke} model, where a semantic hub, not only acts as a pointer and a information-binder, but also constructs, refines semantic concepts and builds cross-modal similarity structures using episodic events. Evidences collected from \emph{semantic dementia} (SD) pathologies~\parencite{nestorDeclarativeMemoryImpairments2006}, cerebral imaging in semantic tasks and brain stimulation experiments~\parencite{pobricCategorySpecificCategoryGeneralSemantic2010} suggest a cross-modal central construction of the semantic hub, activated in linguistic or non-linguistic semantic contrast, regardless of input modality. In parallel, \textcite{paivioMindItsEvolution2008} argued for a dual-coding system to address the problem of representing abstract concepts which do not necessarily have a perceptual input: in addition to accumulated perceptual information for concrete concepts, a more meta-semantic department keeps record of all concepts. In this project we will continue to work with \citeauthor{pattersonWhereYouKnow2007}'s semantic hub.

% \textcite{pattersonWhereYouKnow2007, lambon-ralphNeuralComputationalBases2017} proposed the \emph{Hub-and-Spoke} model. It assumes the core role of language and non-verbal experiences when acquiring and updating concept knowledges, which are stored in modality\slash property-specific cortical areas (such as color, motion), defined as \emph{spokes}.  A mediating transmodal semantic hub is to communicate the modality-dependent information sources, and is argued to be located in the ATL region, concluded from a meta-analysis of studies on semantic processing in human brain, using a variety of imaging methods and task settings. Linguistic, non-linguistic tasks with auditory and visual stimuli, along with generative tasks, all converge to show that bilateral ATL are recruited in object naming, recognition, reading and speech comprehension.

In this project, we restrain the discussion to lexicosemantic system, particularly we will focus on the representation and processing of word meanings in an ecological auditive experiment. 

\section{Syntagmatic and Paradigmatic Axes in Linguistics}
\label{sec:IntroSyntagandParaAxies}
\textcite{jakobsonFundamentalsLanguage1963} and \textcite{desaussureCoursLinguistiqueGenerale1969} propose that all linguistic units are arranged in two modes which are \emph{combination} and \emph{selection}, or \emph{syntagm} and \emph{paradigm}. \emph{Combination} is in \emph{presentia} as the linguistic unit (in the current context, a word\slash lemma\slash lexicon unit) is contextualized by other elements presented in a linguistic sequence. \emph{Selection} is in \emph{absentia} as it is linked to other alternative substitutions which are absent from the current context. Table \ref{tab:egsyntagparadig} gives an example of the organization in two axes.

\begin{table}
    \centering
    \begin{ThreePartTable}  
    \begin{tabularx}{\textwidth}{p{3mm}p{1mm}*{7}{L}}
     & \multicolumn{7}{c}{\tabhead{syntagmatic}} \\
     \parbox[t]{1mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{\tabhead{paradigmatic}}}} & \tikzmark{ps} & \tikzmark{ss} &  &  &  & & &  \tikzmark{se} \\
    & & The & ridiculous & girl & fell & into & the & pond. \\
    & & & \textcolor{gray}{silly} & \textcolor{gray}{person} & \textcolor{gray}{jumped} & & & \textcolor{gray}{river.} \\
    & & & \textcolor{gray}{foolish} & \textcolor{gray}{woman} & \textcolor{gray}{tripped} & & & \textcolor{gray}{lake.} \\
    & & & \textcolor{gray}{funny} & \textcolor{gray}{lady} & \textcolor{gray}{plunged} & & & \textcolor{gray}{sea.} \\
    & & & \textcolor{gray}{crazy} & \textcolor{gray}{princess} & \textcolor{gray}{walked} & & & \textcolor{gray}{ocean.} \\
    & \tikzmark{pe} & & \textcolor{gray}{klutzy} & \textcolor{gray}{child} & \textcolor{gray}{ran} & &  & \textcolor{gray}{pool.} \\
    \end{tabularx}
    \end{ThreePartTable}
    \caption[Example of Syntagmatic and Paradigmatic Axes]{An example of syntagmatic and paradigmatic axes. Gray-colored texts are in \emph{absentia}, black-colored texts are in \emph{presentia}. \emph{Syntagm} combines word sequence into a meaningful sentence, while \emph{paradigm} provides feasible substitutions of currently-present words.\label{tab:egsyntagparadig}}
    \begin{tikzpicture}[overlay, remember picture, yshift=.25\baselineskip, shorten >=.5pt, shorten <=.5pt]
        \draw [->] ([yshift=.75pt]{pic cs:ps}) -- ({pic cs:pe});
        \draw [->] ([yshift=.75pt]{pic cs:ss}) -- ({pic cs:se});
      \end{tikzpicture}
\end{table}


\citeauthor{jakobsonFundamentalsLanguage1963} further illustrated the twofold character of language via selection-deficient and contexture-deficient aphasics using data from \textcite{goldsteinLanguageLanguageDisturbances1948, headAphasiaKindredDisorders1920, hughlingsjacksonAffectionsSpeechDisease1879, goldsteinProblemMeaningWords1971, luriaWorkingBrainIntroduction1976c, crutchAbstractConcreteConcepts2004, warringtonCategorySpecificAccess1983}, bridging formal linguistic works with psycholinguistic studies.

Similarity disorder (selection-deficiency) patients are able to complete scraps of words or sentence, but are unable to comprehend isolated word, to detect one same word in different contexts, and to un-contextualize themselves (they are unable to utter ``it rains'' unless it rains actually). The retrieval of the most precise lexicon is blocked, and the internal relation between concepts are dissolved for those patients. The production of word tends to be bound by other associative words (for example, \emph{knife} are referred to as \emph{pencil-sharpener}, \emph{bread-knife} and \emph{knife-and-fork}) or metonymies (\emph{fork} for \emph{knife}, \emph{eat} for \emph{toaster}), or replaced by the most general terms such as \emph{chose} and \emph{machin} in French. The utterances are highly dominated by spatial, temporal and usage proximities, and the semantic similarity is broken. They also lose the ability to switch register and stay in their idiolect reality. As remarked by \citeauthor{jakobsonFundamentalsLanguage1963}, for such an aphasic whose substitutional capacity has been disabled and contextual capacity intact, the emissive and receptive linguistic competence relies solely on contiguity. 

Contiguity disorder (contexture-deficiency) patients, on the other hand, are impaired to propositionize, inflect and desolve compound words such as \emph{thanksgiving} into \emph{thanks} and \emph{giving}. They produce agrammatical sentences as a chaotic word heap. The approximative identifications of a presented concept are quasimetaphoric (such as \emph{spyglass} are produced for \emph{microscope}, \emph{fire} for \emph{gaslight}), without any deliberate transfer of meaning as it is in the case of poetry and rhetorics. 

%----------------------------------------------------------------------------------------

\section{Computational Semantic Representation Modeling}

Natural language processing and understanding in general artificial intelligence has partially branched away from cognitive computational linguistic works. While language representation models like BERT~\parencite{devlinBERTPretrainingDeep2018} are fine-tuned to natural language processing benchmark tasks (such as GLUE, MultiNLI and SQuAD), they do not necessarily approach human language processing. We restrain computational semantic modeling to the models attempting to replicate of human linguistic dynamics. 

Semantic representation models digitalize the natural language word meanings into numeric representations that can be understood and processed by neural networks and computer systems. There are two schools of models: symbolic and distributional.

\subsection{Symbolic Relational Semantic Models}

\label{subsection:symbolicembedding}
Classical semantic models assume that the meanings can be considered as an indexable binary feature array \parencite{smithSemanticMemoryPsychological1974} or interconnected nodes in a large semantic graph-like ontological network \parencite{collinsRetrievalTimeSemantic1969}. In such structures, the binary features and nodes in the ontologies each represents a semantic entity (\emph{symbol}), to which we associate properties or values. Depending on the implementation, such symbolic structures are usually abstracted or independent from episodic, perceptual experiences. They are able to account for abstract taxonomical conceptual comparisons. Therefore, they model mainly paradigmatic similarities. 

Modern implementation of such models still rely largely on human manual coding. WordNet-alike knowledge bases \parencite{millerWordNetLexicalDatabase1995, millerWordNetElectronicLexical1998, sagotBuildingFreeFrench2008, pradetWonefImprovedExpanded2014} are examples of symbolic semantic networks which encodes inter-word semantic relations. In this class of models, lexicon units are regrouped into \emph{synsets}, forming synonymy sets, each representing one different meaning of the unit. Synsets are interconnected with relations such as \emph{antonymy, hyponymy, hypernymy, meronymy, toponymy}\dots

\subsection{Statistical Distributed Representations}

\label{subsection:statisticalembedding}

\textcite{harrisDistributionalStructure1954}'s distributional hypothesis argues that ``linguistics items with similar distributions have similar meanings.'' Most statistical models based on this theoretical foundation could be classified into latent semantic inference models \parencite{deerwesterUnitedStatesPatent1989, penningtonGloveGlobalVectors2014} and hyperspace analogue to language models \parencite{burgessHyperspaceAnalogueLanguage1995, mikolovEfficientEstimationWord2013, levyDependencyBasedWordEmbeddings2014}. As they make heavy use of contextual information, the syntagmatic information are present in these classes of models.

Such representation models use high dimensional vectors to encode semantic entities. The 2D matrix representation of the model, where the rows are entries of the lexicon, columns being the vector dimensions, are referred to as \emph{semantic embeddings} or \emph{semantic spaces}. Similarity measures are derived from vector distance metrics including cosine distance, gaussian distance and Minkowski distance. Models such as \textcite{penningtonGloveGlobalVectors2014, mikolovEfficientEstimationWord2013} successfully capture semantic information from textual statistics, achieving adequate performance on similarity benchmarks. 

On the linguistic nature of statistical distributed representation (SDR) models, a mixture of syntagmatic and paradigmatic information in statistical embeddings is observed. To give an example, in an openly available GloVe~\parencite{penningtonGloveGlobalVectors2014} implementation\footnote{Wikipedia 2014 + Gigaword 5 with 6B tokens, 400k uncased vocabulary and 300 dimensions. Available at \url{https://nlp.stanford.edu/projects/glove/}.}, the closest neighbors of the target word \emph{teacher} (a noun) are composed of synonyms (\emph{instructor, tutee}) and associates (\emph{classroom, teaching, school, student, aunt\dots}) (Table \ref{tab:gloveneighbours}).  While the list of synonyms proposed by WordNet is \emph{instructor, teaching fellow, docent, coach}, which is purely paradigmatic (synonymy).


\begin{table}
    \centering
    \begin{ThreePartTable}  
    \begin{tabularx}{\textwidth}{*{4}{L}}
    \multicolumn{2}{l}{\tabhead{Target word:}} & teacher & \\
    \toprule
    \tabhead{Neighbour} & \tabhead{Cosine Distance\tnote{1}} & \tabhead{Nature of Neighbour} & \tabhead{Semantic Relation} \\
  \toprule
classroom & 0.537 & associate & locative \\
teaching & 0.497 & associate & action \\
school & 0.484& associate & locative \\
preschool & 0.453 & associate & locative \\
student & 0.421& associate & object/agent\\
grade & 0.418 & associate & \\
college & 0.403 & associate & locative \\
instructor & 0.401 & synonym & \\

    \bottomrule
    \end{tabularx}
    \begin{tablenotes}
        \footnotesize
        \item[*] A cosine distance near 0 indicates a greater similarity.
    \end{tablenotes}  
    \end{ThreePartTable}
    \caption[Example of Syntagmatic and Paradigmatic Mixture in Statistical Semantic Models]{\emph{Teacher} are judged to be close to \emph{classroom, teaching, student}\dots. While they are frequent collocations, they are nevertheless not synonyms. \label{tab:gloveneighbours}}
\end{table}

\textcite{lapesaContrastingSyntagmaticParadigmatic2014} tested combinations of different hyper-parameters of co-occurrence-based statistical representation building algorithm. They used behavioral priming data of syntagmatic and paradigmatic word-pairs to contrast parameters' influence on two axes' performance. Figure \ref{fig:synparacontextwindow} is reproduced using the reported data from the work, confirming more systematically the two-fold mixture in SDR embeddings.

\begin{figure}
    \centering
    \makebox[\linewidth]{
    \includegraphics[scale=1]{Figures/SynParaContextWindow.pdf}
    }
    \caption[Impact of Context Window Size on Syntagmatic and Paradigmatic Information Extraction]{\textcite{lapesaContrastingSyntagmaticParadigmatic2014} tested 6 priming scheme datasets: paradigmatic datasets include synonyms \code{SYN}, antonyms \code{ANT} and cohypernyms \code{COH}, syntagmatic datasets include forward phrasal associates \code{FPA}, backward phrasal associates \code{BPA} and generalized event knowledge \code{GEK}. For each of the 6 datasets, they trained a separate GLM using a set of differently configured semantic embeddings to predict word priming delays. Increasing context window size when training the embedding improves syntagmatic model accuracy, while penalizes paradigmatic predictions. Note that paradigmatic accuracies are consistently better than syntagmatic ones.} 
    \label{fig:synparacontextwindow}
\end{figure}


%----------------------------------------------------------------------------------------

\section{Semantic Neural Encoding\slash Decoding Experiments}

The cognitive account of semantic processing includes the identification of the specific functions of various cortical areas during the semantic process. Historically, to support arguments, neuroscientists had to rely on semantic deficits and lesion studies. With cognitive modeling development, neuroimaging techniques license the examination of various model proposals without having to open the skull. 

\textcite{marrVisionComputationalInvestigation1982} proposed the three levels of modeling in: \emph{computation, algorithm} and \emph{implementation}. On computational level, cognitive modelers either try to replicate the temporal-spatial dynamics of cerebral activities, thus \emph{encodes} neural signals, and/or use measured signals to recover external stimuli, thus \emph{decodes} brain activity. 

Given the temporal and spatial resolution constraints of neuroimaging techniques among electroencephalography (EEG), magnetoencephalography (MEG), functional magnetic resonance imaging (fMRI), positron emission tomography (PET), fMRI and PET gained enormous popularity in the semantic localization studies while MEG for spatial-precise temporal dynamic studies~\parencite{molloOscillatoryDynamicsSupporting2017}. fMRI gives a much better spatial resolution than EEG (up to millimeters), but it is generally poor in temporal resolution. Classic semantic stimuli units are presented at sub-second level (hundreds of milliseconds) but the usual imaging frequency is at second level, and the measured blood-oxygen-level dependent (BOLD) signal approximates a temporal convolution of real neuron activation and hemodynamic function over 4 to 7 seconds. Particularly for semantic encoding, classic fMRI suffers from low signal-to-noise ratio (SNR) for measurements in ventral aspect of the brain due to magnetic field inhomogeneity caused by air-filled sinuses. The problem is partially remedied by multi-echo sequence EPI sequence fMRI.

Semantic encoding and decoding experiments heavily rely on computational semantic representation models, especially distributed representations. \textcite{mitchellPredictingHumanBrain2008} was the first work to use contextual co-occurrence vectors (one variate of statistical distributed representation, SDR) to encode semantic processing activities for concrete nouns. The predictive model showed significant generalization power, indicating a strong association between semantic embeddings and the brain activity, and the feasibility of encoding fMRI recorded semantic brain activity with SDRs.

\subsection{Multi-Network Participation in Semantic Processing}
\label{subsec:multinetworkParticipation}
A large amount of literatures with encoding and decoding experiments report results in favor of distributed semantic processing in the human brain. 

\textcite{mitchellPredictingHumanBrain2008}'s concrete noun encoding experiments reported the most accurately predicted voxels to be located in (pre)frontal, temporoparietal regions. \textcite{pereiraUniversalDecoderLinguistic2018} used fMRI signals to decode lexical and phrasal semantic stimuli presented in isolation. Among the 5000 most informative voxels found for each subject, functional networks in human brain other than the linguistic one take also a significant portion consistently across subjects. 

In addition to experiments conducted with isolated semantic stimuli, works such as \textcite{todorovicAnalysesIRMfLors2018, verdierEncodageActiviteNeuronale2018, huthContinuousSemanticSpace2012} experimented with ecological stimuli. They found consistent encoding\slash decoding performance with non-classically-linguistic cortical area voxels.

Multiple theories exist to account for the contribution of other cortical areas in semantic processing.   

\subsubsection{Evidences for Feature-Based Distribution}
\label{subsec: featurebaseddistribution}
Feature-based distributional models argue that various cortices are recruited to encode modality-specific information~\parencite{chaoAttributebasedNeuralSubstrates1999, haukSomatotopicRepresentationAction2004, goldbergPerceptualKnowledgeRetrieval2006}. The parallel activations of these areas participate in the completion of semantic retrieval and representation~\parencite{pattersonWhereYouKnow2007}. Experiments \parencite{borghesaniWordMeaningVentral2016, moseleySensorimotorSemanticsSpot2013, shtyrovDistributedNeuronalNetworks2004} successfully relate perceptual\slash executive neural signals (size, color, action \dots) with only textual semantic stimuli. \textcite{huthNaturalSpeechReveals2016} proposed a semantic cortical mapping organized by PCA-generated semantic axes including perceptual properties (e.g. visual, tactical, emotional, locational) along with semantic domains(e.g. tools, animals, living animates). \textcite{rowtulaDeepAutoencoderNearPerfect2018} mixed textual semantic embeddings with image-based (visual) semantic embeddings to encode \textcite{pereiraUniversalDecoderLinguistic2018}'s data. The multimodal model, compared to purely textual ones, gave much better predictions on whole-brain BOLD signals.

\subsubsection{Evidences for Semantic-Domain-Based Distribution}

Domain-specific distributional models \parencite{damasioNeuralBasisLexical1996, damasioNeuralSystemsWord2004, mahonWhatDrivesOrganization2011} argue for a cortical map in function of semantic categories (\emph{domains}, such as living animate, vegetables, tools). The argument is mainly supported by category-specific pathology observations: cortical connectivity are locally tuned for different semantic topics' operational processing. 

\textcite{huthContinuousSemanticSpace2012} used WordNet-based noun and verb hierarchical structures to correlate neural responses in different cortical area with word categories (\emph{semantic domain}, e.g. \emph{athlete, communicate}). \textcite{huthNaturalSpeechReveals2016}'s cortical semantic map is illustrated also with domain-specific axes. \textcite{pobricCategorySpecificCategoryGeneralSemantic2010} used transcranial magnetic stimulation to inhibit the left inferior parietal lobule (IPL), resulting to naming difficulties for non-living and high-manipulable objects, but not for living and low-manipulable ones, indicating IPL's role in semantic processing only for concepts of certain domains.

Importantly, semantic features and domains are not necessarily two dissociated principles of semantic organization. For example, domain traits can also imply \emph{feature} information (domestic animals imply the size of the concept in question shall normally not surpass that of an adult human). Similarly, feature specificity can imply domain information, such as visual recognition (for faces) are linked to social contexts.

\subsubsection{Evidences for Semantic Control Networks}
\emph{Controlled Semantic Cognition} \parencite{lambon-ralphNeuralComputationalBases2017} system argues for an operational rather than representational account. CSC is based on the semantic \emph{Hub-and-Spoke} theory~\parencite{pattersonWhereYouKnow2007}, and it considers the neural correlates in non-hub areas as the interaction with semantic representation system and the computation of semantic entities and non-linguistic decision making~\parencite{fusterUpperProcessingStages2004}. Semantic computation, such as combination and selection, is modulated by linguistic and task contexts. The inferior frontal gyrus pars triangularis (IFGtri), posterior middle temporal gyrus (pMTG), angular gyrus and parietal regions are revealed by fMRI and TMS studies to be involved in semantic control \parencite{noonanGoingInferiorPrefrontal2013}. Since language is also a social tool, goal, action and decision making is also implicated in semantic processing. These functions recruit the cortices revealed by semantic encoding/decoding experiments.
%----------------------------------------------------------------------------------------

\section{Outline}

This master's project investigates \citeauthor{pattersonWhereYouKnow2007}'s semantic hub internal organization for concepts and word-meanings. In a more general context, the investigation serves as a discussion on the role of multiple cortices participation in semantic processing\slash representation, especially those which are not classically defined as ``language cortices''. Two principles are proposed for the hub and peripheral components, with their hypothesized functional properties parallel to the paradigmatic and syntagmatic axes proposed by \citeauthor{jakobsonFundamentalsLanguage1963}. The two semantic organization principles are modeled by two types of semantic embeddings. Therefore, the embeddings model the semantic value processed in different cortical regions. The two types of principles are named as \similarity and \association to avoid definition incompatibility, and to license extensions to the semantic content and adaptations of theories in neuroscience and formal linguistics. Hypotheses on the embedding\slash axis construction considerations are presented in Chapter \ref{chap:hypotheses}.

With different semantic embeddings, cortical regions of different operational\slash  representational functions can be underpinned with fMRI encoding, of which the data is acquired in an ecological auditive experiment. The methods of building such embeddings, of validity examinations of the obtained embeddings and of fMRI encoding settings, along with result analyzing schemes are presented in Chapter \ref{chap:methods}.

Results are presented in Chapter \ref{chap:results}, and discussed in Chapter \ref{chap:discussions} along with further post-hoc analyses and potential implications.



% attempts to examine different cortical regions' participation in the two semantic processing axes,which we name as \similarity and \association. We attribute each axis, or organizational principle, with a fine-tuned semantic representation model, configured with our assumptions and hypothetical properties on these axes. 
% Through an fMRI encoding experiment using lexical semantic models in an ecological setting, we infer each region's functional properties based on the locally preferred organizational principle.

% In chapter \ref{chap:hypotheses}, assumptions on \similarity and \association axes and semantic distributional representation structures are presented and discussed along with the semantic memory localization hypotheses. In chapter \ref{chap:methods}, we will present our methods on building semantic representations, preliminary assumption validations, fMRI encoding settings and result interpretation. Then the obtained results are presented in chapter \ref{chap:results}. We will present further ad-hoc analyses and possible implications of our results in chapter \ref{chap:discussions}.